from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime
import subprocess

default_args = {
    'start_date': datetime(2025, 1, 1),
    'retries': 1,
}

def run_script(script_name):
    script_path = f'/opt/airflow/scripts/python/{script_name}'
    result = subprocess.run(['python3', script_path], capture_output=True, text=True)
    print(result.stdout)
    if result.returncode != 0:
        print(result.stderr)
        raise Exception(f"Script {script_name} failed")

with DAG(
    dag_id='my_dag',
    schedule='@daily',
    default_args=default_args,
    catchup=False,
) as dag:

    transform_task = PythonOperator(
        task_id='transform_data',
        python_callable=run_script,
        op_args=['transform_data.py'],
    )

    split_task = PythonOperator(
        task_id='split_data',
        python_callable=run_script,
        op_args=['split_data_into_tables.py'],
    )

    upload_task = PythonOperator(
        task_id='upload_to_s3',
        python_callable=run_script,
        op_args=['upload_data_to_s3_bucket.py'],
    )
    
    load_snowflake_task = PythonOperator(
        task_id='load_into_snowflake',
        python_callable=run_script,
        op_args=['load_into_snowflake.py'],
    )

    transform_task >> split_task >> upload_task >> load_snowflake_task